{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKjgpqeaiTv6",
    "outputId": "290c16da-a464-4295-96c4-16cd94ffc582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 376ms/step\n",
      "Training model f1...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 335ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 341ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Training model f2...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 346ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 345ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 352ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Training model f3...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 356ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 365ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 358ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 361ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Training model f4...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 358ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 355ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 365ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 354ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 351ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Training model f5...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 355ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 366ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 351ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 359ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 369ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 355ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Training model f6...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 350ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 346ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 346ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 344ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 343ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 344ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 361ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Training model f7...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 341ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 343ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 338ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 341ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 354ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 340ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 338ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 336ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Training model f8...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 337ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 342ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 340ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 346ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 340ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 339ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 342ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 340ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 339ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Training model f9...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 341ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 343ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 349ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 351ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 345ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 342ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 342ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 341ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 342ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 345ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Training model f10...\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 350ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 349ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 360ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 343ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 351ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 344ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 350ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 349ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 383ms/step\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 357ms/step\n",
      "Accuracy matrix:\n",
      "[[0.756  0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.756  0.7424 0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.756  0.7424 0.736  0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.756  0.7424 0.736  0.7512 0.     0.     0.     0.     0.     0.    ]\n",
      " [0.756  0.7424 0.736  0.7512 0.754  0.     0.     0.     0.     0.    ]\n",
      " [0.756  0.7424 0.736  0.7512 0.754  0.754  0.     0.     0.     0.    ]\n",
      " [0.756  0.7424 0.736  0.7512 0.754  0.754  0.7336 0.     0.     0.    ]\n",
      " [0.756  0.7424 0.736  0.7512 0.754  0.754  0.7336 0.7548 0.     0.    ]\n",
      " [0.756  0.7424 0.736  0.7512 0.754  0.754  0.7336 0.7548 0.7252 0.    ]\n",
      " [0.756  0.7424 0.736  0.7512 0.754  0.754  0.7336 0.7548 0.7252 0.7596]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_weights(classifier: LwPClassifier, feature_reducer: FeatureReducer, filename: str = \"weights.pkl\"):\n",
    "    \"\"\"Save classifier prototypes and PCA components.\"\"\"\n",
    "    weights = {\n",
    "        \"prototypes\": classifier.prototypes,\n",
    "        \"pca_components\": feature_reducer.pca.components_,\n",
    "        \"pca_mean\": feature_reducer.pca.mean_\n",
    "    }\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(weights, f)\n",
    "    print(f\"Weights saved to {filename}\")\n",
    "\n",
    "def load_weights(classifier: LwPClassifier, feature_reducer: FeatureReducer, filename: str = \"weights.pkl\"):\n",
    "    \"\"\"Load classifier prototypes and PCA components.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        weights = pickle.load(f)\n",
    "    classifier.prototypes = weights[\"prototypes\"]\n",
    "    feature_reducer.pca.components_ = weights[\"pca_components\"]\n",
    "    feature_reducer.pca.mean_ = weights[\"pca_mean\"]\n",
    "    print(f\"Weights loaded from {filename}\")\n",
    "    \n",
    "# Step 1: Load the dataset\n",
    "def load_dataset(filepath: str):\n",
    "    \"\"\"Loads a dataset from a given file path.\"\"\"\n",
    "    t = torch.load(filepath)\n",
    "    data, targets = t['data'], t.get('targets')  # Targets may be None for unlabeled data\n",
    "    return data, targets\n",
    "\n",
    "# Step 2: Feature Extraction with MobileNet\n",
    "class MobileNetFeatureExtractor:\n",
    "    def __init__(self, input_shape=(224, 224, 3)):\n",
    "        # Load MobileNet without the top classification layer\n",
    "        self.model = tf.keras.applications.MobileNet(\n",
    "            weights='imagenet', include_top=False, pooling='avg', input_shape=input_shape\n",
    "        )\n",
    "\n",
    "    def preprocess(self, data: np.ndarray):\n",
    "        \"\"\"Preprocess raw image data to match MobileNet input requirements.\"\"\"\n",
    "        data = data.astype(np.float32)\n",
    "        resized_data = np.array([tf.image.resize(img, (224, 224)).numpy() for img in data])\n",
    "        preprocessed_data = tf.keras.applications.mobilenet.preprocess_input(resized_data)\n",
    "        return preprocessed_data\n",
    "\n",
    "    def extract(self, data: np.ndarray):\n",
    "        \"\"\"Extract features using MobileNet.\"\"\"\n",
    "        preprocessed_data = self.preprocess(data)\n",
    "        features = self.model.predict(preprocessed_data, batch_size=32, verbose=1)\n",
    "        return features\n",
    "\n",
    "# Step 3: Dimensionality Reduction\n",
    "class FeatureReducer:\n",
    "    def __init__(self, n_components=256):\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "\n",
    "    def fit_transform(self, data: np.ndarray):\n",
    "        return self.pca.fit_transform(data)\n",
    "\n",
    "    def transform(self, data: np.ndarray):\n",
    "        return self.pca.transform(data)\n",
    "\n",
    "# Step 4: LwP Classifier\n",
    "class LwPClassifier:\n",
    "    def __init__(self, num_classes: int):\n",
    "        self.num_classes = num_classes\n",
    "        self.prototypes = None\n",
    "\n",
    "    def fit(self, data: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"Initialize prototypes based on labeled data.\"\"\"\n",
    "        self.prototypes = []\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_data = data[labels == cls]\n",
    "            if len(cls_data) > 0:\n",
    "                cls_prototype = cls_data.mean(axis=0)\n",
    "                self.prototypes.append(cls_prototype)\n",
    "        self.prototypes = np.array(self.prototypes)\n",
    "\n",
    "    def predict(self, data: np.ndarray):\n",
    "        \"\"\"Predict labels for the given data.\"\"\"\n",
    "        distances = cdist(data, self.prototypes)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def update(self, data: np.ndarray, pseudo_labels: np.ndarray, alpha=0.7):\n",
    "        \"\"\"Update prototypes using pseudo-labeled data.\"\"\"\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_data = data[pseudo_labels == cls]\n",
    "            if len(cls_data) > 0:\n",
    "                cls_mean = cls_data.mean(axis=0)\n",
    "                self.prototypes[cls] = alpha * self.prototypes[cls] + (1 - alpha) * cls_mean\n",
    "\n",
    "# Step 5: Training and Evaluation\n",
    "def train_and_evaluate(train_files: list, eval_files: list, num_classes=10, alpha=0.7, confidence_threshold=0.9):\n",
    "    \"\"\"Train models f1, ..., f10 and evaluate on held-out datasets.\"\"\"\n",
    "    accuracies = np.zeros((len(train_files), len(eval_files)))\n",
    "    feature_extractor = MobileNetFeatureExtractor()\n",
    "    feature_reducer = FeatureReducer()\n",
    "    classifier = LwPClassifier(num_classes=num_classes)\n",
    "\n",
    "    # Load and prepare D1\n",
    "    data, targets = load_dataset(train_files[0])\n",
    "    features = feature_extractor.extract(data)\n",
    "    reduced_features = feature_reducer.fit_transform(features)\n",
    "    classifier.fit(reduced_features, np.array(targets))\n",
    "\n",
    "    for i in range(1, len(train_files) + 1):\n",
    "        print(f\"Training model f{i}...\")\n",
    "\n",
    "        # Evaluate on held-out datasets\n",
    "        for j in range(i):  # Evaluate only on \\hat{D}_1 to \\hat{D}_i\n",
    "            eval_data, eval_targets = load_dataset(eval_files[j])\n",
    "            eval_features = feature_extractor.extract(eval_data)\n",
    "            eval_reduced_features = feature_reducer.transform(eval_features)\n",
    "            predictions = classifier.predict(eval_reduced_features)\n",
    "            accuracies[i - 1, j] = accuracy_score(eval_targets, predictions)\n",
    "\n",
    "        # Stop after f10\n",
    "        if i == len(train_files):\n",
    "            break\n",
    "\n",
    "        # Load next unlabeled dataset (D2, ..., D10)\n",
    "        next_data, _ = load_dataset(train_files[i])\n",
    "        next_features = feature_extractor.extract(next_data)\n",
    "        next_reduced_features = feature_reducer.transform(next_features)\n",
    "\n",
    "        # Predict labels for next dataset\n",
    "        pseudo_labels = classifier.predict(next_reduced_features)\n",
    "\n",
    "        # Confidence filtering\n",
    "        if confidence_threshold:\n",
    "            distances = cdist(next_reduced_features, classifier.prototypes)\n",
    "            confidence = 1 - (distances.min(axis=1) / distances.max(axis=1))\n",
    "            mask = confidence >= confidence_threshold\n",
    "            next_reduced_features = next_reduced_features[mask]\n",
    "            pseudo_labels = pseudo_labels[mask]\n",
    "\n",
    "        # Update classifier using pseudo-labeled data\n",
    "        classifier.update(next_reduced_features, pseudo_labels, alpha=alpha)\n",
    "        # Save weights after training\n",
    "        save_weights(classifier, feature_reducer, filename=\"trained_weights.pkl\")\n",
    "    return accuracies\n",
    "\n",
    "# Example usage\n",
    "# /content/1_train_data.tar.pth\n",
    "train_files = [f\"dataset/part_one_dataset/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)]  # Replace with actual paths\n",
    "eval_files = [f\"dataset/part_one_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]  # Replace with actual paths\n",
    "accuracies = train_and_evaluate(train_files, eval_files)\n",
    "print(\"Accuracy matrix:\")\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
