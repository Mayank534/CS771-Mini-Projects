{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset D1: Data shape (2500, 32, 32, 3)\n",
      "Dataset D1 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 463ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.8972\n",
      "Training on dataset D2: Data shape (2500, 32, 32, 3)\n",
      "Dataset D2 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 437ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.8488\n",
      "Evaluated on dataset D2: Accuracy = 0.9024\n",
      "Training on dataset D3: Data shape (2500, 32, 32, 3)\n",
      "Dataset D3 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 439ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.7820\n",
      "Evaluated on dataset D2: Accuracy = 0.8576\n",
      "Evaluated on dataset D3: Accuracy = 0.9048\n",
      "Training on dataset D4: Data shape (2500, 32, 32, 3)\n",
      "Dataset D4 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 436ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.7308\n",
      "Evaluated on dataset D2: Accuracy = 0.8072\n",
      "Evaluated on dataset D3: Accuracy = 0.8404\n",
      "Evaluated on dataset D4: Accuracy = 0.8788\n",
      "Training on dataset D5: Data shape (2500, 32, 32, 3)\n",
      "Dataset D5 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 437ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.6856\n",
      "Evaluated on dataset D2: Accuracy = 0.7432\n",
      "Evaluated on dataset D3: Accuracy = 0.7804\n",
      "Evaluated on dataset D4: Accuracy = 0.8352\n",
      "Evaluated on dataset D5: Accuracy = 0.9368\n",
      "Training on dataset D6: Data shape (2500, 32, 32, 3)\n",
      "Dataset D6 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 439ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.6636\n",
      "Evaluated on dataset D2: Accuracy = 0.7088\n",
      "Evaluated on dataset D3: Accuracy = 0.7568\n",
      "Evaluated on dataset D4: Accuracy = 0.8312\n",
      "Evaluated on dataset D5: Accuracy = 0.8996\n",
      "Evaluated on dataset D6: Accuracy = 0.9404\n",
      "Training on dataset D7: Data shape (2500, 32, 32, 3)\n",
      "Dataset D7 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 490ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.6392\n",
      "Evaluated on dataset D2: Accuracy = 0.6708\n",
      "Evaluated on dataset D3: Accuracy = 0.7252\n",
      "Evaluated on dataset D4: Accuracy = 0.7812\n",
      "Evaluated on dataset D5: Accuracy = 0.8716\n",
      "Evaluated on dataset D6: Accuracy = 0.9088\n",
      "Evaluated on dataset D7: Accuracy = 0.9396\n",
      "Training on dataset D8: Data shape (2500, 32, 32, 3)\n",
      "Dataset D8 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 490ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.6116\n",
      "Evaluated on dataset D2: Accuracy = 0.6516\n",
      "Evaluated on dataset D3: Accuracy = 0.7036\n",
      "Evaluated on dataset D4: Accuracy = 0.7588\n",
      "Evaluated on dataset D5: Accuracy = 0.8524\n",
      "Evaluated on dataset D6: Accuracy = 0.8896\n",
      "Evaluated on dataset D7: Accuracy = 0.9164\n",
      "Evaluated on dataset D8: Accuracy = 0.9688\n",
      "Training on dataset D9: Data shape (2500, 32, 32, 3)\n",
      "Dataset D9 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 476ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.5968\n",
      "Evaluated on dataset D2: Accuracy = 0.6272\n",
      "Evaluated on dataset D3: Accuracy = 0.6864\n",
      "Evaluated on dataset D4: Accuracy = 0.7500\n",
      "Evaluated on dataset D5: Accuracy = 0.8284\n",
      "Evaluated on dataset D6: Accuracy = 0.8864\n",
      "Evaluated on dataset D7: Accuracy = 0.8996\n",
      "Evaluated on dataset D8: Accuracy = 0.9232\n",
      "Evaluated on dataset D9: Accuracy = 0.9564\n",
      "Training on dataset D10: Data shape (2500, 32, 32, 3)\n",
      "Dataset D10 does not have targets; using pseudo-labeling.\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 463ms/step\n",
      "Weights saved to trained_weights.pkl\n",
      "Evaluated on dataset D1: Accuracy = 0.5820\n",
      "Evaluated on dataset D2: Accuracy = 0.5968\n",
      "Evaluated on dataset D3: Accuracy = 0.6548\n",
      "Evaluated on dataset D4: Accuracy = 0.7348\n",
      "Evaluated on dataset D5: Accuracy = 0.8044\n",
      "Evaluated on dataset D6: Accuracy = 0.8556\n",
      "Evaluated on dataset D7: Accuracy = 0.8864\n",
      "Evaluated on dataset D8: Accuracy = 0.8976\n",
      "Evaluated on dataset D9: Accuracy = 0.9360\n",
      "Evaluated on dataset D10: Accuracy = 0.9548\n",
      "Accuracy matrix:\n",
      "[[0.8972 0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.8488 0.9024 0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.782  0.8576 0.9048 0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.7308 0.8072 0.8404 0.8788 0.     0.     0.     0.     0.     0.    ]\n",
      " [0.6856 0.7432 0.7804 0.8352 0.9368 0.     0.     0.     0.     0.    ]\n",
      " [0.6636 0.7088 0.7568 0.8312 0.8996 0.9404 0.     0.     0.     0.    ]\n",
      " [0.6392 0.6708 0.7252 0.7812 0.8716 0.9088 0.9396 0.     0.     0.    ]\n",
      " [0.6116 0.6516 0.7036 0.7588 0.8524 0.8896 0.9164 0.9688 0.     0.    ]\n",
      " [0.5968 0.6272 0.6864 0.75   0.8284 0.8864 0.8996 0.9232 0.9564 0.    ]\n",
      " [0.582  0.5968 0.6548 0.7348 0.8044 0.8556 0.8864 0.8976 0.936  0.9548]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle\n",
    "\n",
    "# Load previous weights (prototypes and PCA components)\n",
    "def load_weights(filename: str = \"trained_weights.pkl\"):\n",
    "    \"\"\"Load classifier prototypes and PCA components.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        weights = pickle.load(f)\n",
    "    return weights\n",
    "\n",
    "def save_weights(classifier: LwPClassifier, feature_reducer: FeatureReducer, filename: str = \"weights.pkl\"):\n",
    "    \"\"\"Save classifier prototypes and PCA components.\"\"\"\n",
    "    weights = {\n",
    "        \"prototypes\": classifier.prototypes,\n",
    "        \"pca_components\": feature_reducer.pca.components_,\n",
    "        \"pca_mean\": feature_reducer.pca.mean_\n",
    "    }\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(weights, f)\n",
    "    print(f\"Weights saved to {filename}\")\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "def load_dataset(filepath: str):\n",
    "    \"\"\"Loads a dataset from a given file path.\"\"\"\n",
    "    t = torch.load(filepath)\n",
    "    data, targets = t['data'], t.get('targets')  # Targets may be None for unlabeled data\n",
    "    return data, targets\n",
    "\n",
    "# Step 2: Feature Extraction with ResNet50\n",
    "class ResNet50FeatureExtractor:\n",
    "    def __init__(self, input_shape=(112, 112, 3)):\n",
    "        # Load ResNet50 without the top classification layer\n",
    "        self.model = tf.keras.applications.ResNet50(\n",
    "            weights='imagenet', include_top=False, pooling='avg', input_shape=input_shape\n",
    "        )\n",
    "\n",
    "    def preprocess(self, data: np.ndarray):\n",
    "        \"\"\"Preprocess raw image data to match ResNet50 input requirements.\"\"\"\n",
    "        data = data.astype(np.float32)\n",
    "        resized_data = np.array([tf.image.resize(img, (112, 112)).numpy() for img in data])\n",
    "        preprocessed_data = tf.keras.applications.resnet50.preprocess_input(resized_data)\n",
    "        return preprocessed_data\n",
    "\n",
    "    def extract(self, data: np.ndarray):\n",
    "        \"\"\"Extract features using ResNet50.\"\"\"\n",
    "        preprocessed_data = self.preprocess(data)\n",
    "        features = self.model.predict(preprocessed_data, batch_size=32, verbose=1)\n",
    "        return features\n",
    "\n",
    "# Step 3: Dimensionality Reduction\n",
    "class FeatureReducer:\n",
    "    def __init__(self, n_components=256):\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "\n",
    "    def fit_transform(self, data: np.ndarray):\n",
    "        return self.pca.fit_transform(data)\n",
    "\n",
    "    def transform(self, data: np.ndarray):\n",
    "        return self.pca.transform(data)\n",
    "\n",
    "# Step 4: LwP Classifier with Regularization\n",
    "class LwPClassifier:\n",
    "    def __init__(self, num_classes: int, lambda_reg=0.1):\n",
    "        self.num_classes = num_classes\n",
    "        self.prototypes = None\n",
    "        self.lambda_reg = lambda_reg  # Regularization strength\n",
    "\n",
    "    def fit(self, data: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"Initialize prototypes based on labeled data.\"\"\" \n",
    "        self.prototypes = []\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_data = data[labels == cls]\n",
    "            if len(cls_data) > 0:\n",
    "                cls_prototype = cls_data.mean(axis=0)\n",
    "                self.prototypes.append(cls_prototype)\n",
    "        self.prototypes = np.array(self.prototypes)\n",
    "\n",
    "    def predict(self, data: np.ndarray):\n",
    "        \"\"\"Predict labels for the given data.\"\"\"\n",
    "        distances = cdist(data, self.prototypes)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def update(self, data: np.ndarray, pseudo_labels: np.ndarray, alpha=0.7):\n",
    "        \"\"\"Update prototypes using pseudo-labeled data with regularization.\"\"\"\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_data = data[pseudo_labels == cls]\n",
    "            if len(cls_data) > 0:\n",
    "                cls_mean = cls_data.mean(axis=0)\n",
    "                # Regularized update\n",
    "                self.prototypes[cls] = (alpha * self.prototypes[cls] + (1 - alpha) * cls_mean) - \\\n",
    "                                       self.lambda_reg * (self.prototypes[cls] - cls_mean)\n",
    "\n",
    "def train_and_evaluate(train_files: list, eval_files: list, num_classes=10):\n",
    "    \"\"\"\n",
    "    Train iteratively on datasets (D1, D2, ..., Dn) and evaluate on all datasets after each training step.\n",
    "    Datasets without targets will use pseudo-labels generated by the classifier.\n",
    "    \"\"\"\n",
    "    n = len(train_files)  # Number of datasets\n",
    "    accuracies = np.zeros((n, n))  # Accuracy matrix (n x n): Train on Di, evaluate on Dj\n",
    "    \n",
    "    # Feature extractor and dimensionality reducer\n",
    "    feature_extractor = ResNet50FeatureExtractor()\n",
    "    feature_reducer = FeatureReducer()\n",
    "    classifier = LwPClassifier(num_classes=num_classes)\n",
    "    \n",
    "    # Load previously saved weights if training D11 to D20\n",
    "    weights = load_weights(\"trained_weights.pkl\")\n",
    "    classifier.prototypes = weights[\"prototypes\"]  # Load pre-trained prototypes\n",
    "    feature_reducer.pca.components_ = weights[\"pca_components\"]  # Load pre-trained PCA components\n",
    "    feature_reducer.pca.mean_ = weights[\"pca_mean\"]  # Load pre-trained PCA mean\n",
    "\n",
    "    # Storage for reduced features and targets for reuse during evaluations\n",
    "    reduced_features_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Load training dataset\n",
    "        data, targets = load_dataset(train_files[i])\n",
    "        print(f\"Training on dataset D{i+1}: Data shape {data.shape}\")\n",
    "        \n",
    "        # Check if targets are available\n",
    "        if targets is None:\n",
    "            print(f\"Dataset D{i+1} does not have targets; using pseudo-labeling.\")\n",
    "        \n",
    "        # Extract features\n",
    "        features = feature_extractor.extract(data)\n",
    "        \n",
    "        # Fit PCA on the first dataset and transform all others\n",
    "        if i == 0:\n",
    "            reduced_features = feature_reducer.fit_transform(features)\n",
    "        else:\n",
    "            reduced_features = feature_reducer.transform(features)\n",
    "\n",
    "        # Generate pseudo-labels if targets are not available\n",
    "        if targets is None:\n",
    "            pseudo_labels = classifier.predict(reduced_features)\n",
    "            classifier.update(reduced_features, pseudo_labels)  # Update prototypes using pseudo-labels\n",
    "        else:\n",
    "            targets = np.array(targets)\n",
    "            classifier.fit(reduced_features, targets)  # Fit with actual labels\n",
    "\n",
    "        # Store reduced features and targets/pseudo-labels for future evaluations\n",
    "        reduced_features_list.append(reduced_features)\n",
    "        targets_list.append(targets if targets is not None else pseudo_labels)\n",
    "        # Save weights after training\n",
    "        save_weights(classifier, feature_reducer, filename=\"trained_weights.pkl\")\n",
    "        # Evaluate on all datasets seen so far (D1 to Di)\n",
    "        for j in range(i + 1):\n",
    "            eval_features = reduced_features_list[j]\n",
    "            eval_targets = targets_list[j]\n",
    "            predictions = classifier.predict(eval_features)\n",
    "            accuracies[i, j] = accuracy_score(eval_targets, predictions)\n",
    "            print(f\"Evaluated on dataset D{j+1}: Accuracy = {accuracies[i, j]:.4f}\")\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_files = [f\"dataset/part_two_dataset/train_data/{i}_train_data.tar.pth\" for i in range(1,11)]  # Now using D11 to D20\n",
    "eval_files = [f\"dataset/part_two_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]  # Evaluate on D11 to D20\n",
    "accuracies = train_and_evaluate(train_files, eval_files)\n",
    "print(\"Accuracy matrix:\")\n",
    "print(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
